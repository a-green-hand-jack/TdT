"""
æ™ºèƒ½è§„åˆ™ç”Ÿæˆå™¨

é›†æˆLLM Agentå’Œæ•°æ®åŠ è½½å™¨ï¼Œæä¾›å®Œæ•´çš„è§„åˆ™ç”ŸæˆåŠŸèƒ½ã€‚
ç°å·²æ”¯æŒæ™ºèƒ½åˆ†æ®µå¤„ç†æ¶æ„ï¼Œå¯å¤„ç†å¤æ‚çš„é•¿æƒåˆ©è¦æ±‚ä¹¦ã€‚
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List, Any

from .data_loader import DataLoader
from .llm_agent import LLMRuleAgent
from .claims_splitter import ClaimsSplitter
from .chunked_analyzer import ChunkedAnalyzer
from .result_merger import ResultMerger
from ..models.rule_models import RuleGenerationResult, StandardizedRuleOutput

logger = logging.getLogger(__name__)


class IntelligentRuleGenerator:
    """æ™ºèƒ½è§„åˆ™ç”Ÿæˆå’Œè¾“å‡ºç®¡ç†å™¨"""
    
    def __init__(self, llm_agent: LLMRuleAgent):
        """åˆå§‹åŒ–è§„åˆ™ç”Ÿæˆå™¨
        
        Args:
            llm_agent: LLMè§„åˆ™ç”Ÿæˆæ™ºèƒ½ä½“
        """
        self.llm_agent = llm_agent
        self.data_loader = DataLoader()
        
        # åˆå§‹åŒ–åˆ†æ®µå¤„ç†ç»„ä»¶
        self.claims_splitter = ClaimsSplitter()
        self.chunked_analyzer = ChunkedAnalyzer(llm_agent)
        self.result_merger = ResultMerger()
        
        logger.info("æ™ºèƒ½è§„åˆ™ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒåˆ†æ®µå¤„ç†ï¼‰")
    
    def should_use_chunked_processing(self, claims_data: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨åˆ†æ®µå¤„ç†"""
        if hasattr(claims_data, 'claims'):
            # è®¡ç®—æ€»å†…å®¹é•¿åº¦
            total_content_length = sum(len(claim.content) for claim in claims_data.claims)
            claim_count = len(claims_data.claims)
            
            logger.info(f"ğŸ” æƒåˆ©è¦æ±‚ä¹¦ç»Ÿè®¡:")
            logger.info(f"  ğŸ“ æ€»å†…å®¹é•¿åº¦: {total_content_length}å­—ç¬¦")
            logger.info(f"  ğŸ“‹ æƒåˆ©è¦æ±‚æ•°é‡: {claim_count}ä¸ª")
            
            # è®¡ç®—ä¾èµ–å…³ç³»æ•°é‡
            dependency_count = sum(len(claim.dependencies) for claim in claims_data.claims)
            logger.info(f"  ğŸ”— ä¾èµ–å…³ç³»æ•°é‡: {dependency_count}ä¸ª")
            
            # å¦‚æœæƒåˆ©è¦æ±‚ä¹¦è¶…è¿‡10000å­—ç¬¦ï¼Œå¼ºåˆ¶ä½¿ç”¨åˆ†æ®µå¤„ç†
            if total_content_length > 10000:
                logger.info(f"âœ… æ€»å†…å®¹é•¿åº¦{total_content_length}å­—ç¬¦è¶…è¿‡é˜ˆå€¼ï¼Œå¯ç”¨åˆ†æ®µå¤„ç†")
                return True
            
            # å¦‚æœæƒåˆ©è¦æ±‚æ•°é‡è¶…è¿‡10ä¸ªï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
            if claim_count > 10:
                logger.info(f"âœ… æƒåˆ©è¦æ±‚æ•°é‡{claim_count}ä¸ªè¶…è¿‡é˜ˆå€¼ï¼Œå¯ç”¨åˆ†æ®µå¤„ç†")
                return True
            
            # å¦‚æœå†…å®¹é•¿åº¦è¶…è¿‡5000å­—ç¬¦ä¸”æƒåˆ©è¦æ±‚æ•°é‡è¶…è¿‡5ä¸ªï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
            if total_content_length > 5000 and claim_count > 5:
                logger.info(f"âœ… å¤åˆæ¡ä»¶æ»¡è¶³ï¼ˆé•¿åº¦{total_content_length}å­—ç¬¦ï¼Œæ•°é‡{claim_count}ä¸ªï¼‰ï¼Œå¯ç”¨åˆ†æ®µå¤„ç†")
                return True
            
            # å¦‚æœä¾èµ–å…³ç³»å¤æ‚ï¼ˆè¶…è¿‡20ä¸ªä¾èµ–å…³ç³»ï¼‰ï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
            if dependency_count > 20:
                logger.info(f"âœ… ä¾èµ–å…³ç³»{dependency_count}ä¸ªè¿‡äºå¤æ‚ï¼Œå¯ç”¨åˆ†æ®µå¤„ç†")
                return True
        
        logger.info("ğŸ“ æœªæ»¡è¶³åˆ†æ®µå¤„ç†æ¡ä»¶ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼")
        return False
    
    def _generate_rules_with_chunked_processing(self, 
                                               claims_doc: Any,
                                               existing_rules: Any,
                                               sequence_data: Any) -> RuleGenerationResult:
        """ä½¿ç”¨åˆ†æ®µå¤„ç†ç”Ÿæˆè§„åˆ™"""
        logger.info("ğŸ”§ å¼€å§‹æ™ºèƒ½åˆ†æ®µå¤„ç†")
        
        # 1. åˆå¹¶æƒåˆ©è¦æ±‚ä¹¦å†…å®¹è¿›è¡Œåˆ†æ®µ
        full_claims_text = "\n\n".join([claim.content for claim in claims_doc.claims])
        claim_segments = self.claims_splitter.split_claims(full_claims_text)
        logger.info(f"ğŸ“‹ æƒåˆ©è¦æ±‚ä¹¦åˆ†æ®µå®Œæˆ: {len(claim_segments)}ä¸ªæ®µè½")
        
        # 2. åˆ›å»ºåˆ†æå—
        claim_chunks = self.claims_splitter.create_analysis_chunks(claim_segments, max_chunk_size=3)
        logger.info(f"ğŸ§© åˆ›å»ºåˆ†æå—: {len(claim_chunks)}ä¸ªå—")
        
        # 3. åˆ†å—åˆ†æ
        chunk_results = self.chunked_analyzer.analyze_chunks(
            claim_chunks, 
            sequence_data.model_dump() if hasattr(sequence_data, 'model_dump') else sequence_data,
            existing_rules.rules if hasattr(existing_rules, 'rules') else []
        )
        
        # 4. åˆå¹¶ç»“æœ
        merged_result = self.result_merger.merge_chunk_results(
            chunk_results, 
            claims_doc.patent_number
        )
        
        logger.info(f"âœ… åˆ†æ®µå¤„ç†å®Œæˆ: ç”Ÿæˆ{len(merged_result.merged_rules)}æ¡è§„åˆ™")
        
        # 5. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        return self._convert_chunked_result_to_standard(merged_result, claims_doc)
    
    def _convert_chunked_result_to_standard(self, merged_result: Any, claims_doc: Any) -> RuleGenerationResult:
        """å°†åˆ†æ®µå¤„ç†ç»“æœè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
        from ..models.rule_models import ComplexityLevel, ComplexityAnalysis
        
        # è®¡ç®—å¤æ‚åº¦çº§åˆ«
        if merged_result.total_rules_generated >= 10:
            complexity_level = ComplexityLevel.COMPLEX
        elif merged_result.total_rules_generated >= 5:
            complexity_level = ComplexityLevel.MODERATE
        else:
            complexity_level = ComplexityLevel.SIMPLE
        
        # åˆ›å»ºå¤æ‚åº¦åˆ†æ
        complexity_analysis = ComplexityAnalysis(
            complexity_level=complexity_level,
            complexity_score=min(10.0, merged_result.total_rules_generated * 0.1),
            mutation_count=merged_result.total_rules_generated,
            combination_complexity=min(5, merged_result.total_rules_generated // 10),
            dependency_depth=merged_result.total_claims_analyzed // 50,
            representation_suggestion="ä½¿ç”¨æ™ºèƒ½åˆ†æ®µå¤„ç†æ¶æ„ç”Ÿæˆè¯¦ç»†è§„åˆ™åˆ—è¡¨",
            reasoning=f"åŸºäº{merged_result.total_claims_analyzed}ä¸ªæƒåˆ©è¦æ±‚åˆ†æç”Ÿæˆ{merged_result.total_rules_generated}æ¡è§„åˆ™ï¼Œå¤æ‚åº¦ä¸º{complexity_level.value}"
        )
        
        # åˆ›å»ºç®€åŒ–çš„ç»“æœå¯¹è±¡ï¼Œç»•è¿‡å¤æ‚çš„æ•°æ®æ¨¡å‹éªŒè¯
        # å¯¹äºåˆ†æ®µå¤„ç†ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ç®€åŒ–æ ¼å¼
        class SimplifiedRuleGenerationResult:
            def __init__(self, patent_number, merged_rules, total_claims_analyzed, total_rules_generated, analysis_summary, quality_metrics):
                from datetime import datetime
                self.patent_number = patent_number
                self.generation_timestamp = datetime.now()
                self.llm_model = "qwen3-max-preview"
                self.protection_rules = merged_rules  # ç®€åŒ–è§„åˆ™åˆ—è¡¨
                self.chunked_rules = merged_rules  # ğŸ¯ åˆ†æ®µå¤„ç†ç”Ÿæˆçš„130æ¡è§„åˆ™
                self.total_claims_analyzed = total_claims_analyzed
                self.total_rules_generated = total_rules_generated
                self.complexity_analysis = complexity_analysis
                self.avoidance_strategies = []
                self.llm_reasoning = f"æ™ºèƒ½åˆ†æ®µå¤„ç†: åˆ†æ{total_claims_analyzed}ä¸ªæƒåˆ©è¦æ±‚ï¼Œç”Ÿæˆ{total_rules_generated}æ¡è§„åˆ™"
                self.analysis_summary = analysis_summary
                self.analysis_confidence = quality_metrics.get("rule_quality", {}).get("avg_quality_score", 0.8)
                self.raw_llm_response = json.dumps(merged_rules, ensure_ascii=False, indent=2)
                self.processing_log = []
                
        return SimplifiedRuleGenerationResult(
            patent_number=claims_doc.patent_number,
            merged_rules=merged_result.merged_rules,
            total_claims_analyzed=merged_result.total_claims_analyzed,
            total_rules_generated=merged_result.total_rules_generated,
            analysis_summary=merged_result.analysis_summary,
            quality_metrics=merged_result.quality_metrics
        )
    
    def generate_rules_from_patent(self, 
                                 claims_path: str,
                                 sequence_json_path: str,
                                 existing_rules_json_path: str) -> RuleGenerationResult:
        """ä»ä¸“åˆ©æ•°æ®ç”Ÿæˆå®Œæ•´çš„ä¿æŠ¤è§„åˆ™
        
        Args:
            claims_path: æƒåˆ©è¦æ±‚ä¹¦Markdownæ–‡ä»¶è·¯å¾„
            sequence_json_path: æ ‡å‡†åŒ–åºåˆ—JSONæ–‡ä»¶è·¯å¾„
            existing_rules_json_path: ç°æœ‰è§„åˆ™JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            è§„åˆ™ç”Ÿæˆç»“æœ
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆè§„åˆ™: {claims_path}")
        
        try:
            # åŠ è½½æ•°æ®
            claims_doc = self.data_loader.load_claims_markdown(Path(claims_path))
            sequence_data = self.data_loader.load_sequence_json(Path(sequence_json_path))
            existing_rules = self.data_loader.load_existing_rules(Path(existing_rules_json_path))
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {claims_doc.patent_number}")
            
            # åˆ›å»ºåºåˆ—ä¸æƒåˆ©è¦æ±‚çš„æ˜ å°„
            sequence_mapping = self.data_loader.create_sequence_claims_mapping(
                claims_doc, sequence_data
            )
            
            logger.info(f"åºåˆ—æ˜ å°„å®Œæˆ: {sequence_mapping.mapping_statistics}")
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨åˆ†æ®µå¤„ç†
            if self.should_use_chunked_processing(claims_doc):
                logger.info("ğŸ”„ å¯ç”¨æ™ºèƒ½åˆ†æ®µå¤„ç†æ¨¡å¼")
                result = self._generate_rules_with_chunked_processing(
                    claims_doc, existing_rules, sequence_data
                )
            else:
                logger.info("ğŸ“ ä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼")
                # ä½¿ç”¨LLMè¿›è¡Œåˆ†æ
                result = self.llm_agent.analyze_patent_claims(
                    claims_doc, existing_rules, sequence_data
                )
            
            # æ·»åŠ æ˜ å°„ä¿¡æ¯åˆ°ç»“æœä¸­
            result.analysis_summary.update({
                'sequence_mapping': sequence_mapping.mapping_statistics,
                'total_mapped_sequences': len(sequence_mapping.sequence_to_claims),
                'unmatched_seq_ids': len(sequence_mapping.unmatched_seq_ids)
            })
            
            logger.info(f"è§„åˆ™ç”Ÿæˆå®Œæˆ: {claims_doc.patent_number}")
            return result
            
        except Exception as e:
            logger.error(f"è§„åˆ™ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def export_to_json(self, rules: RuleGenerationResult, output_path: str) -> None:
        """å¯¼å‡ºJSONæ ¼å¼è§„åˆ™æ–‡ä»¶
        
        Args:
            rules: è§„åˆ™ç”Ÿæˆç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢ä¸ºæ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼
        standardized_output = StandardizedRuleOutput(
            metadata={
                "patent_number": rules.patent_number,
                "generation_timestamp": rules.generation_timestamp.isoformat(),
                "llm_model": rules.llm_model,
                "analysis_confidence": rules.analysis_confidence,
                "total_rules": len(rules.protection_rules)
            },
            rules=rules.protection_rules,
            analysis_summary=rules.analysis_summary,
            generation_info={
                "complexity_analysis": rules.complexity_analysis.dict(),
                "avoidance_strategies": [strategy.dict() for strategy in rules.avoidance_strategies],
                "llm_reasoning": rules.llm_reasoning,
                "processing_log": rules.processing_log
            }
        )
        
        # å¯¼å‡ºJSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(
                standardized_output.dict(),
                f,
                ensure_ascii=False,
                indent=2
            )
        
        logger.info(f"JSONè§„åˆ™æ–‡ä»¶å·²å¯¼å‡º: {output_path}")
    
    def export_simplified_json(self, result: RuleGenerationResult, output_path: str,
                              raw_llm_response: Optional[str] = None) -> None:
        """å¯¼å‡ºç®€åŒ–JSONæ ¼å¼è§„åˆ™æ–‡ä»¶
        
        Args:
            result: è§„åˆ™ç”Ÿæˆç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            raw_llm_response: åŸå§‹LLMå“åº”ï¼Œç”¨äºå®¹é”™å¤„ç†
        """
        logger.info(f"å¯¼å‡ºç®€åŒ–JSONè§„åˆ™åˆ°: {output_path}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†æ®µå¤„ç†çš„ç»“æœï¼ˆSimplifiedRuleGenerationResultï¼‰
            logger.info(f"ğŸ” æ£€æŸ¥ç»“æœç±»å‹: {type(result).__name__}")
            logger.info(f"ğŸ” æ˜¯å¦æœ‰chunked_ruleså±æ€§: {hasattr(result, 'chunked_rules')}")
            if hasattr(result, 'chunked_rules'):
                logger.info(f"ğŸ” chunked_rulesé•¿åº¦: {len(result.chunked_rules) if result.chunked_rules else 0}")
            
            if hasattr(result, 'chunked_rules') and result.chunked_rules:
                logger.info(f"ğŸ¯ å¯¼å‡ºåˆ†æ®µå¤„ç†çš„{len(result.chunked_rules)}æ¡è§„åˆ™")
                simplified_rules = {
                    "patent_number": result.patent_number,
                    "group": 1,
                    "processing_method": "æ™ºèƒ½åˆ†æ®µå¤„ç†",
                    "rules": result.chunked_rules,
                    "metadata": {
                        "total_rules": len(result.chunked_rules),
                        "processing_timestamp": datetime.now().isoformat(),
                        "claims_analyzed": getattr(result, 'total_claims_analyzed', 0),
                        "analysis_confidence": getattr(result, 'analysis_confidence', 0.8)
                    }
                }
            else:
                # æ ‡å‡†å¤„ç†è·¯å¾„ï¼šå°è¯•ä»LLMç»“æœä¸­æå–ç®€åŒ–æ ¼å¼
                simplified_rules = self._extract_simplified_rules(result, raw_llm_response)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥ç®€åŒ–JSONæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(simplified_rules, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ç®€åŒ–JSONè§„åˆ™æ–‡ä»¶å·²ä¿å­˜: {output_path}")
            if hasattr(result, 'chunked_rules'):
                logger.info(f"ğŸ“Š æˆåŠŸå¯¼å‡º {len(result.chunked_rules)} æ¡è§„åˆ™")
            
        except Exception as e:
            logger.error(f"ç®€åŒ–JSONå¯¼å‡ºå¤±è´¥: {e}")
            # å®¹é”™ï¼šä¿å­˜å¤‡ç”¨æ ¼å¼
            self._save_fallback_json(result, output_path, raw_llm_response)
    
    def _extract_simplified_rules(self, result: RuleGenerationResult, 
                                 raw_llm_response: Optional[str] = None) -> Dict:
        """ä»LLMç»“æœä¸­æå–ç®€åŒ–è§„åˆ™æ ¼å¼
        
        Args:
            result: è§„åˆ™ç”Ÿæˆç»“æœ
            raw_llm_response: åŸå§‹LLMå“åº”
            
        Returns:
            ç®€åŒ–çš„è§„åˆ™å­—å…¸
        """
        # å°è¯•ä»åŸå§‹å“åº”ä¸­è§£æç®€åŒ–æ ¼å¼
        if raw_llm_response:
            try:
                # æ¸…ç†å¹¶è§£æåŸå§‹å“åº”
                cleaned_response = self._clean_json_response(raw_llm_response)
                parsed_response = json.loads(cleaned_response)
                
                # å¦‚æœå“åº”å·²ç»æ˜¯ç®€åŒ–æ ¼å¼ï¼Œç›´æ¥è¿”å›
                if "rules" in parsed_response and isinstance(parsed_response["rules"], list):
                    return parsed_response
                    
            except Exception as e:
                logger.warning(f"åŸå§‹å“åº”è§£æå¤±è´¥: {e}")
        
        # å¦‚æœæ— æ³•ä»åŸå§‹å“åº”è§£æï¼Œæ„é€ ç®€åŒ–æ ¼å¼
        return {
            "patent_number": result.patent_number,
            "group": 1,
            "rules": [
                {
                    "wild_type": "SEQ_ID_NO_1",
                    "rule": "analysis_completed",
                    "mutation": "å¾…åˆ†æ", 
                    "statement": f"å·²å®Œæˆ{result.patent_number}çš„ä¿æŠ¤è§„åˆ™åˆ†æ",
                    "comment": "ç³»ç»Ÿç”Ÿæˆçš„å¤‡ç”¨æ ¼å¼"
                }
            ]
        }
    
    def _clean_json_response(self, response: str) -> str:
        """æ¸…ç†JSONå“åº”ä¸­çš„markdownæ ‡è®°
        
        Args:
            response: åŸå§‹å“åº”
            
        Returns:
            æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
        """
        import re
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        patterns = [
            r'```json\s*(.*?)\s*```',
            r'```\s*(.*?)\s*```'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        return response.strip()
    
    def _save_fallback_json(self, result: RuleGenerationResult, output_path: str,
                           raw_llm_response: Optional[str] = None) -> None:
        """ä¿å­˜å¤‡ç”¨JSONæ ¼å¼
        
        Args:
            result: è§„åˆ™ç”Ÿæˆç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            raw_llm_response: åŸå§‹LLMå“åº”
        """
        try:
            fallback_data = {
                "patent_number": result.patent_number,
                "group": 1,
                "status": "parsing_fallback",
                "rules": [
                    {
                        "wild_type": "è§£æå¤±è´¥",
                        "rule": "parsing_failed",
                        "mutation": "N/A",
                        "statement": "LLMå“åº”è§£æå¤±è´¥ï¼Œéœ€è¦äººå·¥å¤„ç†",
                        "comment": "å¤‡ç”¨è¾“å‡ºæ ¼å¼"
                    }
                ],
                "raw_response_preview": raw_llm_response[:500] if raw_llm_response else "æ— åŸå§‹å“åº”",
                "timestamp": datetime.now().isoformat()
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(fallback_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"å¤‡ç”¨JSONæ–‡ä»¶å·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨JSONä¿å­˜ä¹Ÿå¤±è´¥: {e}")
    
    def export_to_markdown(self, rules: RuleGenerationResult, output_path: str,
                          simplified_data: Optional[Dict] = None) -> None:
        """å¯¼å‡ºç®€åŒ–Markdownæ ¼å¼è¯´æ˜æ–‡æ¡£
        
        Args:
            rules: è§„åˆ™ç”Ÿæˆç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            simplified_data: ç®€åŒ–çš„è§„åˆ™æ•°æ®
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç®€åŒ–Markdownå†…å®¹
        md_content = self._generate_simplified_markdown_content(rules, simplified_data)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        logger.info(f"Markdownæ–‡æ¡£å·²å¯¼å‡º: {output_path}")
    
    def _generate_simplified_markdown_content(self, rules: RuleGenerationResult, 
                                            simplified_data: Optional[Dict] = None) -> str:
        """ç”Ÿæˆç®€åŒ–çš„Markdownå†…å®¹
        
        Args:
            rules: è§„åˆ™ç”Ÿæˆç»“æœ
            simplified_data: ç®€åŒ–çš„è§„åˆ™æ•°æ®
            
        Returns:
            Markdownå†…å®¹å­—ç¬¦ä¸²
        """
        md_content = f"""# ä¸“åˆ©ä¿æŠ¤è§„åˆ™åˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **ä¸“åˆ©å·**: {rules.patent_number}
- **åˆ†ææ—¶é—´**: {rules.generation_timestamp.strftime('%Y-%m-%d %H:%M:%S')}
- **åˆ†ææ¨¡å‹**: {rules.llm_model}

## ä¿æŠ¤è§„åˆ™è¯†åˆ«

"""
        
        # å¦‚æœæœ‰ç®€åŒ–æ•°æ®ï¼Œä½¿ç”¨ç®€åŒ–æ ¼å¼
        if simplified_data and "rules" in simplified_data:
            rule_count = len(simplified_data["rules"])
            md_content += f"å…±è¯†åˆ«å‡º **{rule_count}** æ¡ä¿æŠ¤è§„åˆ™ã€‚\n\n"
            
            for i, rule in enumerate(simplified_data["rules"], 1):
                md_content += f"""### è§„åˆ™ {i}

- **é‡ç”Ÿå‹åºåˆ—**: {rule.get('wild_type', 'N/A')}
- **ä¿æŠ¤ç±»å‹**: {rule.get('rule', 'N/A')}
- **çªå˜æ¨¡å¼**: {rule.get('mutation', 'N/A')}

**ä¿æŠ¤æè¿°**: {rule.get('statement', 'N/A')}

**ç­–ç•¥è¯´æ˜**: {rule.get('comment', 'N/A')}

"""
                
                # å¦‚æœæœ‰é€»è¾‘è¡¨è¾¾å¼ï¼Œæ˜¾ç¤ºå®ƒä»¬
                if rule.get('mutation_logic'):
                    md_content += f"**çªå˜é€»è¾‘**: `{rule['mutation_logic']}`\n\n"
                if rule.get('identity_logic'):
                    md_content += f"**åŒä¸€æ€§é€»è¾‘**: `{rule['identity_logic']}`\n\n"
        else:
            # ä½¿ç”¨å¤‡ç”¨æ ¼å¼
            md_content += """### è§„åˆ™åˆ†æç»“æœ

è¯¥ä¸“åˆ©çš„ä¿æŠ¤è§„åˆ™æ­£åœ¨åˆ†æä¸­ï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒJSONè¾“å‡ºæ–‡ä»¶ã€‚

"""
        
        md_content += f"""## åˆ†ææ€»ç»“

æœ¬æŠ¥å‘Šè¯†åˆ«äº† **{rules.patent_number}** ä¸“åˆ©å¯¹TDTé…¶åºåˆ—çš„ä¿æŠ¤èŒƒå›´ã€‚

### å…³é”®å‘ç°

- ä¸“åˆ©ä¿æŠ¤é‡‡ç”¨äº†ç»“æ„åŒ–çš„çªå˜æ¨¡å¼æè¿°
- ä¿æŠ¤è§„åˆ™ä½¿ç”¨é€»è¾‘è¡¨è¾¾å¼ä¾¿äºç¨‹åºå¤„ç†
- åˆ†æèšç„¦äºåºåˆ—ä¿æŠ¤èŒƒå›´çš„è¯†åˆ«

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*Agentç‰ˆæœ¬: ç®€åŒ–ä¿æŠ¤è§„åˆ™åˆ†æ*
"""
        
        return md_content
    
    def generate_analysis_report(self, rules: RuleGenerationResult) -> str:
        """ç”Ÿæˆè§„åˆ™åˆ†ææŠ¥å‘Š
        
        Args:
            rules: è§„åˆ™ç”Ÿæˆç»“æœ
            
        Returns:
            åˆ†ææŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        
        report_lines.append(f"# ä¸“åˆ©è§„åˆ™åˆ†ææŠ¥å‘Š")
        report_lines.append(f"**ä¸“åˆ©å·**: {rules.patent_number}")
        report_lines.append(f"**åˆ†ææ—¶é—´**: {rules.generation_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"**ä½¿ç”¨æ¨¡å‹**: {rules.llm_model}")
        report_lines.append(f"**åˆ†æç½®ä¿¡åº¦**: {rules.analysis_confidence:.2%}")
        report_lines.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        report_lines.append("## åŸºæœ¬ç»Ÿè®¡")
        report_lines.append(f"- ä¿æŠ¤è§„åˆ™æ•°é‡: {len(rules.protection_rules)}")
        report_lines.append(f"- å¤æ‚åº¦çº§åˆ«: {rules.complexity_analysis.complexity_level}")
        report_lines.append(f"- å¤æ‚åº¦è¯„åˆ†: {rules.complexity_analysis.complexity_score:.1f}/10")
        report_lines.append(f"- å›é¿ç­–ç•¥æ•°é‡: {len(rules.avoidance_strategies)}")
        report_lines.append("")
        
        # ä¿æŠ¤è§„åˆ™æ‘˜è¦
        if rules.protection_rules:
            report_lines.append("## ä¿æŠ¤è§„åˆ™æ‘˜è¦")
            for i, rule in enumerate(rules.protection_rules, 1):
                report_lines.append(f"### è§„åˆ™ {i}: {rule.rule_id}")
                report_lines.append(f"- **ç±»å‹**: {rule.rule_type}")
                report_lines.append(f"- **ä¿æŠ¤èŒƒå›´**: {rule.protection_scope}")
                report_lines.append(f"- **å¤æ‚åº¦**: {rule.complexity_score:.1f}/10")
                report_lines.append(f"- **æŠ€æœ¯æè¿°**: {rule.technical_description}")
                report_lines.append("")
        
        # å›é¿ç­–ç•¥
        if rules.avoidance_strategies:
            report_lines.append("## æ¨èå›é¿ç­–ç•¥")
            for i, strategy in enumerate(rules.avoidance_strategies, 1):
                report_lines.append(f"### ç­–ç•¥ {i}: {strategy.strategy_type}")
                report_lines.append(f"**æè¿°**: {strategy.description}")
                report_lines.append(f"**ç½®ä¿¡åº¦**: {strategy.confidence_score:.2%}")
                report_lines.append("**å®æ–½å»ºè®®**:")
                for suggestion in strategy.implementation_suggestions:
                    report_lines.append(f"- {suggestion}")
                report_lines.append(f"**é£é™©è¯„ä¼°**: {strategy.risk_assessment}")
                report_lines.append("")
        
        # åˆ†ææ€»ç»“
        report_lines.append("## åˆ†ææ€»ç»“")
        if rules.analysis_summary:
            for key, value in rules.analysis_summary.items():
                report_lines.append(f"- **{key}**: {value}")
        
        return "\n".join(report_lines)
    
    def _generate_markdown_content(self, rules: RuleGenerationResult) -> str:
        """ç”ŸæˆMarkdownå†…å®¹"""
        lines = []
        
        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        lines.append(f"# ä¸“åˆ©ä¿æŠ¤è§„åˆ™åˆ†æ")
        lines.append("")
        lines.append("## åŸºæœ¬ä¿¡æ¯")
        lines.append("")
        lines.append(f"**ä¸“åˆ©å·**: {rules.patent_number}")
        lines.append(f"**åˆ†ææ—¶é—´**: {rules.generation_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"**LLMæ¨¡å‹**: {rules.llm_model}")
        lines.append(f"**åˆ†æç½®ä¿¡åº¦**: {rules.analysis_confidence:.2%}")
        lines.append("")
        
        # å¤æ‚åº¦åˆ†æ
        lines.append("## å¤æ‚åº¦åˆ†æ")
        lines.append("")
        lines.append(f"**å¤æ‚åº¦çº§åˆ«**: {rules.complexity_analysis.complexity_level}")
        lines.append(f"**å¤æ‚åº¦è¯„åˆ†**: {rules.complexity_analysis.complexity_score:.1f}/10")
        lines.append(f"**çªå˜ä½ç‚¹æ•°é‡**: {rules.complexity_analysis.mutation_count}")
        lines.append(f"**è¡¨è¾¾å»ºè®®**: {rules.complexity_analysis.representation_suggestion}")
        lines.append("")
        lines.append(f"**åˆ¤æ–­ç†ç”±**: {rules.complexity_analysis.reasoning}")
        lines.append("")
        
        # ä¿æŠ¤è§„åˆ™è¯¦æƒ…
        if rules.protection_rules:
            lines.append("## ä¿æŠ¤è§„åˆ™è¯¦æƒ…")
            lines.append("")
            
            for i, rule in enumerate(rules.protection_rules, 1):
                lines.append(f"### è§„åˆ™ {i}: {rule.rule_id}")
                lines.append("")
                lines.append(f"**è§„åˆ™ç±»å‹**: {rule.rule_type}")
                lines.append(f"**ä¿æŠ¤èŒƒå›´**: {rule.protection_scope}")
                lines.append(f"**å¤æ‚åº¦è¯„åˆ†**: {rule.complexity_score:.1f}/10")
                lines.append(f"**ç›®æ ‡åºåˆ—**: {', '.join(rule.target_sequences)}")
                
                if rule.identity_threshold:
                    lines.append(f"**ç›¸ä¼¼åº¦é˜ˆå€¼**: {rule.identity_threshold:.1%}")
                
                lines.append("")
                lines.append(f"**æ³•å¾‹æè¿°**: {rule.legal_description}")
                lines.append("")
                lines.append(f"**æŠ€æœ¯æè¿°**: {rule.technical_description}")
                lines.append("")
                
                if rule.mutation_combinations:
                    lines.append("**çªå˜ç»„åˆ**:")
                    for combo in rule.mutation_combinations:
                        lines.append(f"- {combo.pattern_description}")
                        lines.append(f"  - çªå˜: {', '.join([m.mutation_code for m in combo.mutations])}")
                        lines.append(f"  - ç±»å‹: {combo.combination_type}")
                    lines.append("")
        
        # å›é¿ç­–ç•¥
        if rules.avoidance_strategies:
            lines.append("## å›é¿ç­–ç•¥")
            lines.append("")
            
            for i, strategy in enumerate(rules.avoidance_strategies, 1):
                lines.append(f"### ç­–ç•¥ {i}: {strategy.strategy_type}")
                lines.append("")
                lines.append(f"**æè¿°**: {strategy.description}")
                lines.append(f"**ç½®ä¿¡åº¦**: {strategy.confidence_score:.2%}")
                lines.append("")
                lines.append("**å®æ–½å»ºè®®**:")
                for suggestion in strategy.implementation_suggestions:
                    lines.append(f"- {suggestion}")
                lines.append("")
                lines.append(f"**é£é™©è¯„ä¼°**: {strategy.risk_assessment}")
                lines.append("")
        
        # åˆ†ææ‘˜è¦
        lines.append("## åˆ†ææ‘˜è¦")
        lines.append("")
        if rules.analysis_summary:
            for key, value in rules.analysis_summary.items():
                if isinstance(value, dict):
                    lines.append(f"**{key}**:")
                    for sub_key, sub_value in value.items():
                        lines.append(f"- {sub_key}: {sub_value}")
                else:
                    lines.append(f"**{key}**: {value}")
                lines.append("")
        
        # é¡µè„š
        lines.append("---")
        lines.append("")
        lines.append("*æ­¤æ–‡æ¡£ç”±TDTä¸“åˆ©åºåˆ—è§„åˆ™æå–å·¥å…·è‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(lines)
    
    @classmethod
    def create_with_qwen(cls, api_key: Optional[str] = None, model: str = "qwen-plus") -> 'IntelligentRuleGenerator':
        """åˆ›å»ºä½¿ç”¨Qwençš„è§„åˆ™ç”Ÿæˆå™¨
        
        Args:
            api_key: Qwen APIå¯†é’¥
            model: æ¨¡å‹åç§°
            
        Returns:
            æ™ºèƒ½è§„åˆ™ç”Ÿæˆå™¨å®ä¾‹
        """
        llm_agent = LLMRuleAgent(api_key=api_key, model=model)
        return cls(llm_agent)
